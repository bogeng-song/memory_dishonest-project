{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from openpyxl import Workbook\n",
    "\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import f_oneway\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.stats import ttest_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "plt.rcParams['text.color'] = 'black'\n",
    "plt.rcParams['axes.labelcolor'] = 'black'\n",
    "plt.rcParams['xtick.color'] = 'black'\n",
    "plt.rcParams['ytick.color'] = 'black'\n",
    "plt.rcParams['legend.facecolor'] = 'white' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File read and organize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reward = pd.read_excel('reward_quest.xlsx')\n",
    "df_punish = pd.read_excel('punishment_quest.xlsx')\n",
    "df_mixed = pd.read_excel('mixed_quest.xlsx')\n",
    "df_p_memory1 = pd.read_csv('punishment_memory_summary.csv')\n",
    "df_r_memory1 = pd.read_csv('reward_memory_summary.csv')\n",
    "df_m_memory1 = pd.read_csv('mixed_memory_summary.csv')\n",
    "df_m_fmri = pd.read_csv('mixed_fmribehav_summary.csv')\n",
    "df_r_fmri = pd.read_csv('reward_fmribehav_summary.csv')\n",
    "df_p_fmri = pd.read_csv('punishment_fmribehav_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p_memory1['rt'] = pd.to_numeric(df_p_memory1['rt'], errors='coerce')\n",
    "df_p_memory1 = df_p_memory1.dropna(subset=['rt'])\n",
    "df_r_memory1['rt'] = pd.to_numeric(df_r_memory1['rt'], errors='coerce')\n",
    "df_r_memory1 = df_r_memory1.dropna(subset=['rt'])\n",
    "df_m_memory1['rt'] = pd.to_numeric(df_m_memory1['rt'], errors='coerce')\n",
    "df_m_memory1 = df_m_memory1.dropna(subset=['rt'])\n",
    "\n",
    "df_m_memory1['meta'] = 1 - (df_m_memory1['accuracy'] - df_m_memory1['confidence']/100)**2\n",
    "df_r_memory1['meta'] = 1 - (df_r_memory1['accuracy'] - df_r_memory1['confidence']/100)**2\n",
    "df_p_memory1['meta'] = 1 - (df_p_memory1['accuracy'] - df_p_memory1['confidence']/100)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r_memory = df_r_memory1.groupby(['subject_nr', 'menu', 'stage', 'condition']).agg(\n",
    "    acc=('accuracy', 'mean'),\n",
    "    conf=('confidence', 'mean'),\n",
    "    meta=('meta', 'mean'),\n",
    "    AUC=('AUC', 'mean'),\n",
    "    RT=('rt', 'mean'),\n",
    "    MAD=('MAD', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "df_m_memory = df_m_memory1.groupby(['subject_nr', 'menu', 'stage', 'condition']).agg(\n",
    "    acc=('accuracy', 'mean'),\n",
    "    conf=('confidence', 'mean'),\n",
    "    meta=('meta', 'mean'),\n",
    "    AUC=('AUC', 'mean'),\n",
    "    RT=('rt', 'mean'),\n",
    "    MAD=('MAD', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "df_p_memory = df_p_memory1.groupby(['subject_nr', 'menu', 'stage', 'condition']).agg(\n",
    "    acc=('accuracy', 'mean'),\n",
    "    conf=('confidence', 'mean'),\n",
    "    meta=('meta', 'mean'),\n",
    "    AUC=('AUC', 'mean'),\n",
    "    RT=('rt', 'mean'),\n",
    "    MAD=('MAD', 'mean')\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_list_reward = df_r_memory['subject_nr'].unique()\n",
    "cond_list_reward = df_r_memory['condition'].unique()\n",
    "stage_list_reward = df_r_memory['stage'].unique()\n",
    "\n",
    "sub_list_punishment = df_p_memory['subject_nr'].unique()\n",
    "cond_list_punishment = df_p_memory['condition'].unique()\n",
    "stage_list_punishment = df_p_memory['stage'].unique()\n",
    "\n",
    "sub_list_mixed = df_m_memory['subject_nr'].unique()\n",
    "cond_list_mixed= df_m_memory['condition'].unique()\n",
    "stage_list_mixed= df_m_memory['stage'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_r_fmri['reward_prob_1'] = df_r_fmri['money_prob_1']\n",
    "df_r_fmri['reward_prob_2'] = df_r_fmri['money_prob_2']\n",
    "df_p_fmri['reward_prob_1'] = 10 - df_p_fmri['electric_prob_1']\n",
    "df_p_fmri['reward_prob_2'] = 10 - df_p_fmri['electric_prob_2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the lie-rate and lie-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_lierate(fmridata, sub_list):\n",
    "    # Filter data\n",
    "    fmridata = fmridata[fmridata['subj'].isin(sub_list)]\n",
    "    fmridata = fmridata[fmridata['rt'] <= 4]\n",
    "    fmridata.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Initialize reward_sign column\n",
    "    fmridata['reward_sign'] = -1\n",
    "\n",
    "    # Calculate reward_sign based on menu positions and probabilities\n",
    "    for i in range(len(fmridata)):\n",
    "        menu_position_1 = fmridata.iloc[i]['menu_position_1']\n",
    "        reward_prob_1 = fmridata.iloc[i]['reward_prob_1']\n",
    "        reward_prob_2 = fmridata.iloc[i]['reward_prob_2']\n",
    "        \n",
    "        if menu_position_1 > 36:\n",
    "            if reward_prob_1 > reward_prob_2:\n",
    "                fmridata.loc[i, 'reward_sign'] = 1  # Using .loc to avoid SettingWithCopyWarning\n",
    "            elif reward_prob_1 < reward_prob_2:\n",
    "                fmridata.loc[i, 'reward_sign'] = 0\n",
    "        else:\n",
    "            if reward_prob_1 > reward_prob_2:\n",
    "                fmridata.loc[i, 'reward_sign'] = 0\n",
    "            elif reward_prob_1 < reward_prob_2:\n",
    "                fmridata.loc[i, 'reward_sign'] = 1\n",
    "\n",
    "    # Calculate reward_gain based on choices\n",
    "    fmridata['reward_gain'] = None\n",
    "    for i in range(len(fmridata)):\n",
    "        menu_position_1 = fmridata.iloc[i]['menu_position_1']\n",
    "        menu_position_2 = fmridata.iloc[i]['menu_position_2']\n",
    "        choose = fmridata.iloc[i]['choose']\n",
    "        \n",
    "        if pd.notna(menu_position_1) and pd.notna(choose) and menu_position_1 == choose:\n",
    "            fmridata.loc[i, 'reward_gain'] = fmridata.iloc[i]['reward_prob_1']\n",
    "        elif pd.notna(menu_position_2) and pd.notna(choose) and menu_position_2 == choose:\n",
    "            fmridata.loc[i, 'reward_gain'] = fmridata.iloc[i]['reward_prob_2']\n",
    "            \n",
    "    return fmridata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_entropy(fmridata):   \n",
    "    # Calculate lie-rate for each subject, menu, and condition\n",
    "    agg_menu = fmridata.groupby(['subj', 'menu', 'condition']).agg(\n",
    "        lierate=('islie', 'mean'),\n",
    "        scan_rt=('rt', lambda x: np.mean(pd.to_numeric(x, errors='coerce'))),\n",
    "        reward_sign=('reward_sign', 'mean'),\n",
    "        reward_gain_sum=('reward_gain', 'sum')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Calculate average lie-rate per subject and condition\n",
    "    agg_entropy = agg_menu.groupby(['subj', 'condition']).agg(\n",
    "        lierate=('lierate', 'mean')\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Calculate entropy (information theory measure of uncertainty)\n",
    "    agg_entropy['entropy'] = -agg_entropy['lierate'] * np.log2(agg_entropy['lierate']) - \\\n",
    "                            (1 - agg_entropy['lierate']) * np.log2(1 - agg_entropy['lierate'])\n",
    "    \n",
    "    # Handle edge cases where lierate is 0 or 1\n",
    "    agg_entropy.loc[agg_entropy['lierate'] == 1, 'entropy'] = 0\n",
    "    agg_entropy.loc[agg_entropy['lierate'] == 0, 'entropy'] = 0\n",
    "    \n",
    "    return agg_entropy, agg_menu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p_fmri2 = calculate_lierate(df_p_fmri, sub_list_punishment)\n",
    "df_r_fmri2 = calculate_lierate(df_r_fmri, sub_list_reward)\n",
    "df_m_fmri2 = calculate_lierate(df_m_fmri, sub_list_mixed)\n",
    "\n",
    "# Calculate lie-rates and entropy\n",
    "agg_p_lie_entro, agg_p_menu = agg_entropy(df_p_fmri2)\n",
    "agg_r_lie_entro, agg_r_menu = agg_entropy(df_r_fmri2)\n",
    "agg_m_lie_entro, agg_m_menu = agg_entropy(df_m_fmri2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the lie-rate and entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_total_condition(agg_lie_entro):\n",
    "    total_lie_rate = agg_lie_entro.groupby('subj')['lierate'].mean().reset_index()\n",
    "    total_lie_rate['condition'] = 'total'\n",
    "    combined = pd.concat([agg_lie_entro, total_lie_rate], ignore_index=True)\n",
    "    # Reorder conditions\n",
    "    combined['condition'] = pd.Categorical(combined['condition'], categories=['total'] + list(agg_lie_entro['condition'].unique()), ordered=True)\n",
    "    return combined\n",
    "\n",
    "# Add the 'total' condition to each dataset\n",
    "agg_p_lie_entro_total = add_total_condition(agg_p_lie_entro)\n",
    "agg_r_lie_entro_total = add_total_condition(agg_r_lie_entro)\n",
    "agg_m_lie_entro_total = add_total_condition(agg_m_lie_entro)\n",
    "\n",
    "def plot_lie_rate_with_strip(agg_lie_entro, title):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot KDE for each condition\n",
    "    for i, condition in enumerate(['total', 'enhance_honesty', 'enhance_lie', 'enhance_random']):\n",
    "        subset = agg_lie_entro[agg_lie_entro['condition'] == condition]\n",
    "        kde = gaussian_kde(subset['lierate'])\n",
    "        #y_values = np.linspace(0, 1, 1000)\n",
    "        y_values = np.linspace(min(subset['lierate']), max(subset['lierate']), 1000)\n",
    "        kde_values = kde(y_values)\n",
    "        kde_values = kde_values / kde_values.max() * 0.2 \n",
    "        plt.fill_betweenx(y_values, i + 0.2, kde_values + i + 0.2, color='pink', alpha=0.8)\n",
    "        \n",
    "\n",
    "    sns.boxplot(data=agg_lie_entro, x='condition', y='lierate', whis=[0, 100], width=0.2, \n",
    "                showcaps=False, boxprops={'facecolor':'None'}, showmeans=True, meanprops={\"marker\":\"o\",\n",
    "                \"markerfacecolor\":\"black\", \"markeredgecolor\":\"black\", \"markersize\":\"5\"})\n",
    "    \n",
    "\n",
    "    sns.stripplot(data=agg_lie_entro, x='condition', y='lierate', color='pink', jitter=True, size=5, alpha=0.8)\n",
    "    \n",
    "    #plt.xlim(-0.5, len(agg_lie_entro['condition'].unique()) - 0.5 + 0.5)\n",
    "    plt.title(title, fontsize=24)\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('Probability of lying', fontsize=16)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(rotation=0, fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_lie_rate_with_strip(agg_p_lie_entro_total, 'Punishment')\n",
    "plot_lie_rate_with_strip(agg_r_lie_entro_total, 'Reward')\n",
    "plot_lie_rate_with_strip(agg_m_lie_entro_total, 'Mixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lierate_lines(agg_p, agg_r, agg_m):\n",
    "\n",
    "    conditions = ['enhance_lie', 'enhance_random', 'enhance_honesty']\n",
    "    colors = ['red', 'green', 'blue']\n",
    "    labels = ['Punishment', 'Reward', 'Mixed']\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for i, (agg, label) in enumerate(zip([agg_p, agg_r, agg_m], labels)):\n",
    "        means = [agg[agg['condition'] == condition]['lierate'].mean() for condition in conditions]\n",
    "        errors = [agg[agg['condition'] == condition]['lierate'].sem() for condition in conditions]\n",
    "        \n",
    "        plt.errorbar(conditions, means, yerr=errors, marker='o', color=colors[i], label=label, capsize=5)\n",
    "    \n",
    "    #plt.title('Lierate Across Conditions', fontsize=16)\n",
    "    #plt.xlabel('Condition', fontsize=12)\n",
    "    plt.ylabel('Probability of lying', fontsize=12)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.legend(title='Condition')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_lierate_lines(agg_p_lie_entro_total, agg_r_lie_entro_total, agg_m_lie_entro_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the memory change and plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metric(data, metric, sub_list, cond_list, stage_list):\n",
    "    result = np.zeros((len(sub_list), len(cond_list), len(stage_list)))\n",
    "    \n",
    "    for i, sub in enumerate(sub_list):\n",
    "        sub_data = data[data['subject_nr'] == sub]\n",
    "        for j, cond in enumerate(cond_list):\n",
    "            cond_data = sub_data[sub_data['condition'] == cond]\n",
    "            for k, stage in enumerate(stage_list):\n",
    "                stage_data = cond_data[cond_data['stage'] == stage]\n",
    "                result[i, j, k] = stage_data[metric].mean()\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "cond_list = ['enhance_honesty', 'enhance_lie', 'enhance_random']\n",
    "stage_list = ['pre', 'post', 'post2']\n",
    "\n",
    "rt_reward = calculate_metric(df_r_memory, 'RT', sub_list_reward, cond_list, stage_list)\n",
    "acc_reward = calculate_metric(df_r_memory, 'acc', sub_list_reward, cond_list, stage_list)\n",
    "confidence_reward = calculate_metric(df_r_memory, 'conf', sub_list_reward, cond_list, stage_list)\n",
    "mad_reward = calculate_metric(df_r_memory, 'MAD', sub_list_reward, cond_list, stage_list)\n",
    "auc_reward = calculate_metric(df_r_memory, 'AUC', sub_list_reward, cond_list, stage_list)\n",
    "meta_reward = calculate_metric(df_r_memory, 'meta', sub_list_reward, cond_list, stage_list)\n",
    "\n",
    "\n",
    "rt_punish = calculate_metric(df_p_memory, 'RT', sub_list_punishment, cond_list, stage_list)\n",
    "acc_punish = calculate_metric(df_p_memory, 'acc', sub_list_punishment, cond_list, stage_list)\n",
    "confidence_punish = calculate_metric(df_p_memory, 'conf', sub_list_punishment, cond_list, stage_list)\n",
    "mad_punish = calculate_metric(df_p_memory, 'MAD', sub_list_punishment, cond_list, stage_list)\n",
    "auc_punish = calculate_metric(df_p_memory, 'AUC', sub_list_punishment, cond_list, stage_list)\n",
    "meta_punish = calculate_metric(df_p_memory, 'meta', sub_list_punishment, cond_list, stage_list)\n",
    "\n",
    "rt_mixed = calculate_metric(df_m_memory, 'RT', sub_list_mixed, cond_list, stage_list)\n",
    "acc_mixed = calculate_metric(df_m_memory, 'acc', sub_list_mixed, cond_list, stage_list)\n",
    "confidence_mixed = calculate_metric(df_m_memory, 'conf', sub_list_mixed, cond_list, stage_list)\n",
    "mad_mixed = calculate_metric(df_m_memory, 'MAD', sub_list_mixed, cond_list, stage_list)\n",
    "auc_mixed = calculate_metric(df_m_memory, 'AUC', sub_list_mixed, cond_list, stage_list)\n",
    "meta_mixed = calculate_metric(df_m_memory, 'meta', sub_list_mixed, cond_list, stage_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the behavior performance and plot it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_behavior_bar_two_stages(data, cond_list, stage_list, title , ylimit, y_label):\n",
    "\n",
    "    data = data[:, :, :2]\n",
    "\n",
    "    means = np.mean(data, axis=0)\n",
    "    errors = np.std(data, axis=0) / np.sqrt(data.shape[0])\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    bar_width = 0.3\n",
    "    for i, stage in enumerate(stage_list[:2]):\n",
    "        plt.bar(np.arange(len(cond_list)) + i * bar_width, means[:, i], yerr=errors[:, i], \n",
    "                width=bar_width, label=stage, capsize=5)\n",
    "    \n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Condition', fontsize=12)\n",
    "    plt.ylabel(y_label, fontsize=12)\n",
    "    plt.xticks(np.arange(len(cond_list))+ bar_width / 2, cond_list, fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.ylim(ylimit)\n",
    "    plt.legend(title='Stage')\n",
    "    #plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttest_behavior_two_stages(data, parameter_name):\n",
    "    data = data[:, :, :2]\n",
    "    t_value, p_value = ttest_rel(data[:, 0, 0], data[:, 0, 1])\n",
    "    if p_value < 0.05:\n",
    "        print(f'{parameter_name} honesty condition. t-value: {t_value}, p-value: {p_value}')\n",
    "    t_value, p_value = ttest_rel(data[:, 1, 0], data[:, 1, 1])\n",
    "    if p_value < 0.05:\n",
    "        print(f'{parameter_name} lie condition. t-value: {t_value}, p-value: {p_value}')\n",
    "    t_value, p_value = ttest_rel(data[:, 2, 0], data[:, 2, 1])\n",
    "    if p_value < 0.05:\n",
    "        print(f'{parameter_name} random condition. t-value: {t_value}, p-value: {p_value}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the t-test score for each parameter\n",
    "'''\n",
    "ttest_behavior_two_stages(acc_reward, 'Accuracy Reward')\n",
    "ttest_behavior_two_stages(rt_reward, 'Reaction Time Reward')\n",
    "ttest_behavior_two_stages(confidence_reward, 'Confidence Reward')\n",
    "ttest_behavior_two_stages(mad_reward, 'MAD Reward')\n",
    "ttest_behavior_two_stages(auc_reward, 'AUC Reward')\n",
    "ttest_behavior_two_stages(meta_reward, 'Meta Reward')\n",
    "\n",
    "ttest_behavior_two_stages(acc_punish, 'Accuracy Punishment')\n",
    "ttest_behavior_two_stages(rt_punish, 'Reaction Time Punishment')\n",
    "ttest_behavior_two_stages(confidence_punish, 'Confidence Punishment')\n",
    "ttest_behavior_two_stages(mad_punish, 'MAD Punishment')\n",
    "ttest_behavior_two_stages(auc_punish, 'AUC Punishment')\n",
    "ttest_behavior_two_stages(meta_punish, 'Meta Punishment')\n",
    "\n",
    "ttest_behavior_two_stages(acc_mixed, 'Accuracy Mixed')\n",
    "ttest_behavior_two_stages(rt_mixed, 'RT Mixed')\n",
    "ttest_behavior_two_stages(confidence_mixed, 'Confidence Mixed')\n",
    "ttest_behavior_two_stages(mad_mixed, 'MAD Mixed')\n",
    "ttest_behavior_two_stages(auc_mixed, 'AUC Mixed')\n",
    "ttest_behavior_two_stages(meta_mixed, 'Meta Mixed')\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the behavior result\n",
    "'''\n",
    "plot_behavior_bar_two_stages(acc_reward, cond_list, stage_list, 'Reward',[0.85,1],'Accuracy')\n",
    "plot_behavior_bar_two_stages(acc_punish, cond_list, stage_list, 'Punishment',[0.85,1],'Accuracy')\n",
    "plot_behavior_bar_two_stages(acc_mixed, cond_list, stage_list, 'Mixed',[0.85,1],'Accuracy')\n",
    "\n",
    "plot_behavior_bar_two_stages(rt_reward, cond_list, stage_list, 'Reward',[950,1350],'Reaction Time(ms)')\n",
    "plot_behavior_bar_two_stages(rt_punish, cond_list, stage_list, 'Punishment',[950,1350],'Reaction Time(ms)')\n",
    "plot_behavior_bar_two_stages(rt_mixed, cond_list, stage_list, 'Mixed',[950,1350],'Reaction Time(ms)')\n",
    "\n",
    "plot_behavior_bar_two_stages(confidence_reward, cond_list, stage_list, 'Reward',[85,105],'Confidence')\n",
    "plot_behavior_bar_two_stages(confidence_punish, cond_list, stage_list, 'Punishment',[85,105],'Confidence')\n",
    "plot_behavior_bar_two_stages(confidence_mixed, cond_list, stage_list, 'Mixed',[85,105],'Confidence')\n",
    "\n",
    "plot_behavior_bar_two_stages(meta_reward, cond_list, stage_list, 'Reward',[0.85,1.05],'Meta Confidence')\n",
    "plot_behavior_bar_two_stages(meta_punish, cond_list, stage_list, 'Punishment',[0.85,1.05],'Meta Confidence')\n",
    "plot_behavior_bar_two_stages(meta_mixed, cond_list, stage_list, 'Mixed',[0.85,1.05],'Meta Confidence')\n",
    "\n",
    "plot_behavior_bar_two_stages(auc_reward, cond_list, stage_list, 'Reward',[0.2,0.5],'AUC')\n",
    "plot_behavior_bar_two_stages(auc_punish, cond_list, stage_list, 'Punishment',[0.2,0.5],'AUC')\n",
    "plot_behavior_bar_two_stages(auc_mixed, cond_list, stage_list, 'Mixed',[0.2,0.5],'AUC')\n",
    "\n",
    "plot_behavior_bar_two_stages(mad_reward, cond_list, stage_list, 'Reward',[0.4,0.8],'MAD value')\n",
    "plot_behavior_bar_two_stages(mad_punish, cond_list, stage_list, 'Punishment',[0.4,0.8],'MAD value')\n",
    "plot_behavior_bar_two_stages(mad_mixed, cond_list, stage_list, 'Mixed',[0.4,0.8],'MAD value')\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the Emotional value from three study and get the correlation figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_correlation(df, col1, col2, xlabel, ylabel):\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    \n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "    sns.scatterplot(data=df, x=col1, y=col2, color='white', alpha=0.7, s=80, marker=\"o\", edgecolor='blue', linewidth=2)\n",
    "    sns.regplot(data=df, x=col1, y=col2, scatter=False, color='blue', line_kws={\"linewidth\": 2})\n",
    "    \n",
    "    plt.xlabel(xlabel, fontsize=18)\n",
    "    plt.ylabel(ylabel, fontsize=18)\n",
    "\n",
    "    plt.grid(visible=True, linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name_list_p = df_punish.columns.tolist()\n",
    "reward_emotion_list = [20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42]\n",
    "punish_emotion_list = [26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48]\n",
    "mixed_emotion_list = [26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48]\n",
    "\n",
    "emotion_list = ['Interested','Distressed','Excited','Upset','Strong','Guilty','Scared','Hostile','Enthusiastic','Proud','Irritable','Alert','Ashamed','Inspired','Nervous','Determined','Attentive','Jittery','Active','Afraid','Pos','Neg','Total']\n",
    "\n",
    "\n",
    "col_nn = col_name_list_p[punish_emotion_list[0]]\n",
    "emo_value_p = df_punish[col_nn]\n",
    "data_punish = np.zeros([len(sub_list_punishment), len(emotion_list)])\n",
    "\n",
    "for i in range(len(sub_list_punishment)):\n",
    "    for j in range(len(emotion_list)):\n",
    "        col_nn = col_name_list_p[punish_emotion_list[j]]\n",
    "        data_punish[i,j] = df_punish[col_nn][df_punish.iloc[:, 6] == sub_list_punishment[i]].to_numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "beh_list = ['lierate_honesty','lie_entropy_honesty','acc_drop_honesty','meta_drop_honesty',\n",
    "            'rt_drop_honesty','conf_drop_honesty','auc_drop_honesty',\n",
    "            'lierate_dishonest','lie_entropy_dishonest','acc_drop_dishonest','meta_drop_dishonest',\n",
    "            'rt_drop_dishonest','conf_drop_dishonest','auc_drop_dishonest',\n",
    "            'lierate_random','lie_entropy_random','acc_drop_random','meta_drop_random',\n",
    "            'rt_drop_random','conf_drop_random','auc_drop_random']\n",
    "data_punish_beh = np.zeros([len(sub_list_punishment), len(beh_list)])\n",
    "\n",
    "cond_list_name = ['enhance_honesty','enhance_lie','enhance_random']\n",
    "\n",
    "for k in range(3):   \n",
    "    for i in range(len(sub_list_punishment)):\n",
    "        agg_sub = agg_p_lie_entro[agg_p_lie_entro['subj'] == sub_list_punishment[i]]\n",
    "        data_punish_beh[i,k*7+0] = agg_sub[agg_sub['condition'] == cond_list_name[k]]['lierate'].to_numpy()[0]\n",
    "        data_punish_beh[i,k*7+1] = agg_sub[agg_sub['condition'] == cond_list_name[k]]['entropy'].to_numpy()[0]\n",
    "\n",
    "    cond_index = k\n",
    "    data_punish_beh[:,k*7+3] = meta_punish[:,cond_index,0] - meta_punish[:,cond_index,1]\n",
    "    data_punish_beh[:,k*7+2] = acc_punish[:,cond_index,0] - acc_punish[:,cond_index,1]\n",
    "    data_punish_beh[:,k*7+4] = np.abs(rt_punish[:,cond_index,0] - rt_punish[:,cond_index,1])\n",
    "    data_punish_beh[:,k*7+5] = confidence_punish[:,cond_index,0] - confidence_punish[:,cond_index,1]\n",
    "    data_punish_beh[:,k*7+6] = auc_punish[:,cond_index,0] - auc_punish[:,cond_index,1]\n",
    "    \n",
    "data_punish_total = np.concatenate((data_punish, data_punish_beh), axis=1)\n",
    "column_list = ['Interested','Distressed','Excited','Upset',\n",
    "               'Strong','Guilty','Scared','Hostile',\n",
    "               'Enthusiastic','Proud','Irritable','Alert',\n",
    "               'Ashamed','Inspired','Nervous','Determined',\n",
    "               'Attentive','Jittery','Active','Afraid',\n",
    "               'Pos','Neg','Total',\n",
    "               'lierate_honesty','lie_entropy_honesty','acc_drop_honesty','meta_drop_honesty',\n",
    "               'rt_drop_honesty','conf_drop_honesty','auc_drop_honesty',\n",
    "               'lierate_dishonest','lie_entropy_dishonest','acc_drop_dishonest','meta_drop_dishonest',\n",
    "               'rt_drop_dishonest','conf_drop_dishonest','auc_drop_dishonest',\n",
    "               'lierate_random','lie_entropy_random','acc_drop_random','meta_drop_random',\n",
    "               'rt_drop_random','conf_drop_random','auc_drop_random']\n",
    "df_punish_final = pd.DataFrame(data_punish_total, columns=column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reward = np.zeros([len(sub_list_reward), len(emotion_list)])\n",
    "\n",
    "for i in range(len(sub_list_reward)):\n",
    "    for j in range(len(emotion_list)):\n",
    "        col_nn = col_name_list_p[punish_emotion_list[j]]\n",
    "        data_reward[i,j] = df_reward[col_nn][df_reward.iloc[:, 1] == sub_list_reward[i]].to_numpy()[0]\n",
    "        \n",
    "beh_list = ['lierate_honesty','lie_entropy_honesty','acc_drop_honesty','meta_drop_honesty',\n",
    "            'rt_drop_honesty','conf_drop_honesty','auc_drop_honesty',\n",
    "            'lierate_dishonest','lie_entropy_dishonest','acc_drop_dishonest','meta_drop_dishonest',\n",
    "            'rt_drop_dishonest','conf_drop_dishonest','auc_drop_dishonest',\n",
    "            'lierate_random','lie_entropy_random','acc_drop_random','meta_drop_random',\n",
    "            'rt_drop_random','conf_drop_random','auc_drop_random']\n",
    "\n",
    "data_beh = np.zeros([len(sub_list_reward), len(beh_list)])\n",
    "\n",
    "cond_list_name = ['enhance_honesty','enhance_lie','enhance_random']\n",
    "for k in range(3):\n",
    "    for i in range(len(sub_list_reward)):\n",
    "        agg_sub = agg_r_lie_entro[agg_r_lie_entro['subj'] == sub_list_reward[i]]\n",
    "        data_beh[i,k*7+0] = agg_sub[agg_sub['condition'] == cond_list_name[k]]['lierate'].to_numpy()[0]\n",
    "        data_beh[i,k*7+1] = agg_sub[agg_sub['condition'] == cond_list_name[k]]['entropy'].to_numpy()[0]\n",
    "\n",
    "    cond_index = k\n",
    "    data_beh[:,k*7+3] = meta_reward[:,cond_index,0] - meta_reward[:,cond_index,1]\n",
    "    data_beh[:,k*7+2] = acc_reward[:,cond_index,0] - acc_reward[:,cond_index,1]\n",
    "    data_beh[:,k*7+4] = np.abs(rt_reward[:,cond_index,0] - rt_reward[:,cond_index,1])\n",
    "    data_beh[:,k*7+5] = confidence_reward[:,cond_index,0] - confidence_reward[:,cond_index,1]\n",
    "    data_beh[:,k*7+6] = auc_reward[:,cond_index,0] - auc_reward[:,cond_index,1]\n",
    "\n",
    "data_total = np.concatenate((data_reward, data_beh), axis=1)\n",
    "column_list = ['Interested','Distressed','Excited','Upset',\n",
    "               'Strong','Guilty','Scared','Hostile',\n",
    "               'Enthusiastic','Proud','Irritable','Alert',\n",
    "               'Ashamed','Inspired','Nervous','Determined',\n",
    "               'Attentive','Jittery','Active','Afraid',\n",
    "               'Pos','Neg','Total',\n",
    "               'lierate_honesty','lie_entropy_honesty','acc_drop_honesty','meta_drop_honesty',\n",
    "               'rt_drop_honesty','conf_drop_honesty','auc_drop_honesty',\n",
    "               'lierate_dishonest','lie_entropy_dishonest','acc_drop_dishonest','meta_drop_dishonest',\n",
    "               'rt_drop_dishonest','conf_drop_dishonest','auc_drop_dishonest',\n",
    "               'lierate_random','lie_entropy_random','acc_drop_random','meta_drop_random',\n",
    "               'rt_drop_random','conf_drop_random','auc_drop_random']\n",
    "df_reward_final = pd.DataFrame(data_total, columns=column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mixed = np.zeros([len(sub_list_mixed), len(emotion_list)])\n",
    "\n",
    "for i in range(len(sub_list_mixed)):\n",
    "    for j in range(len(emotion_list)):\n",
    "        col_nn = col_name_list_p[punish_emotion_list[j]]\n",
    "        data_mixed[i,j] = df_mixed[col_nn][df_mixed.iloc[:, 6] == sub_list_mixed[i]].to_numpy()[0]\n",
    "        \n",
    "beh_list = ['lierate_honesty','lie_entropy_honesty','acc_drop_honesty','meta_drop_honesty',\n",
    "            'rt_drop_honesty','conf_drop_honesty','auc_drop_honesty',\n",
    "            'lierate_dishonest','lie_entropy_dishonest','acc_drop_dishonest','meta_drop_dishonest',\n",
    "            'rt_drop_dishonest','conf_drop_dishonest','auc_drop_dishonest',\n",
    "            'lierate_random','lie_entropy_random','acc_drop_random','meta_drop_random',\n",
    "            'rt_drop_random','conf_drop_random','auc_drop_random']\n",
    "data_beh = np.zeros([len(sub_list_mixed), len(beh_list)])\n",
    "\n",
    "cond_list_name = ['enhance_honesty','enhance_lie','enhance_random']\n",
    "for k in range(3):\n",
    "    for i in range(len(sub_list_mixed)):\n",
    "        agg_sub = agg_m_lie_entro[agg_m_lie_entro['subj'] == sub_list_mixed[i]]\n",
    "        data_beh[i,k*7+0] = agg_sub[agg_sub['condition'] == cond_list_name[k]]['lierate'].to_numpy()[0]\n",
    "        data_beh[i,k*7+1] = agg_sub[agg_sub['condition'] == cond_list_name[k]]['entropy'].to_numpy()[0]\n",
    "\n",
    "    cond_index = k\n",
    "    data_beh[:,k*7+3] = meta_mixed[:,cond_index,0] - meta_mixed[:,cond_index,1]\n",
    "    data_beh[:,k*7+2] = acc_mixed[:,cond_index,0] - acc_mixed[:,cond_index,1]\n",
    "    data_beh[:,k*7+4] = np.abs(rt_mixed[:,cond_index,0] - rt_mixed[:,cond_index,1])\n",
    "    data_beh[:,k*7+5] = confidence_mixed[:,cond_index,0] - confidence_mixed[:,cond_index,1]\n",
    "    data_beh[:,k*7+6] = auc_mixed[:,cond_index,0] - auc_mixed[:,cond_index,1]\n",
    "\n",
    "data_total = np.concatenate((data_mixed, data_beh), axis=1)\n",
    "column_list = ['Interested','Distressed','Excited','Upset',\n",
    "               'Strong','Guilty','Scared','Hostile',\n",
    "               'Enthusiastic','Proud','Irritable','Alert',\n",
    "               'Ashamed','Inspired','Nervous','Determined',\n",
    "               'Attentive','Jittery','Active','Afraid',\n",
    "               'Pos','Neg','Total',\n",
    "               'lierate_honesty','lie_entropy_honesty','acc_drop_honesty','meta_drop_honesty',\n",
    "               'rt_drop_honesty','conf_drop_honesty','auc_drop_honesty',\n",
    "               'lierate_dishonest','lie_entropy_dishonest','acc_drop_dishonest','meta_drop_dishonest',\n",
    "               'rt_drop_dishonest','conf_drop_dishonest','auc_drop_dishonest',\n",
    "               'lierate_random','lie_entropy_random','acc_drop_random','meta_drop_random',\n",
    "               'rt_drop_random','conf_drop_random','auc_drop_random']\n",
    "df_mixed_final = pd.DataFrame(data_total, columns=column_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "plot_correlation(df_punish_final, 'Distressed', 'acc_drop_dishonest', 'Distressed Value','Accuracy Value Drop')\n",
    "plot_correlation(df_punish_final, 'Distressed', 'meta_drop_dishonest', 'Distressed Value','Meta-Confidence Value Drop')\n",
    "plot_correlation(df_reward_final, 'Distressed', 'auc_drop_dishonest', 'Distressed Value','AUC Value Drop')\n",
    "plot_correlation(df_reward_final, 'Jittery', 'auc_drop_dishonest', 'Jittery Value','AUC Value Drop')\n",
    "plot_correlation(df_mixed_final, 'Distressed', 'meta_drop_dishonest', 'Distressed Value','Meta-Confidence Value Drop')\n",
    "plot_correlation(df_mixed_final, 'Strong', 'auc_drop_dishonest', 'Strong Value','AUC Value Drop')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare and plot and difference among three conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_and_plot(col_name):\n",
    "\n",
    "    data_reward = df_reward_final[col_name]\n",
    "    data_punish = df_punish_final[col_name]\n",
    "    data_mixed = df_mixed_final[col_name]\n",
    "    \n",
    "    combined_df = pd.DataFrame({\n",
    "        'Reward': data_reward,\n",
    "        'Punishment': data_punish,\n",
    "        'Mixed': data_mixed\n",
    "    })\n",
    "\n",
    "    melted_df = combined_df.melt(var_name='Condition', value_name=col_name)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='Condition', y=col_name, data=melted_df, palette='Set2')\n",
    "    sns.stripplot(x='Condition', y=col_name, data=melted_df, color='black', jitter=True, alpha=0.5)\n",
    "    \n",
    "    plt.title(f'Comparison of {col_name} Across Conditions', fontsize=16)\n",
    "    plt.xlabel('Condition', fontsize=12)\n",
    "    plt.ylabel(col_name, fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anova_and_plot(col_name,ylabel):\n",
    "\n",
    "    data_reward = df_reward_final[col_name]\n",
    "    data_punish = df_punish_final[col_name]\n",
    "    data_mixed = df_mixed_final[col_name]\n",
    "    \n",
    "\n",
    "    anova_result = f_oneway(data_reward, data_punish, data_mixed)\n",
    "    print(f\"ANOVA result for {col_name}: F={anova_result.statistic}, p={anova_result.pvalue}\")\n",
    "    \n",
    "\n",
    "    combined_data = pd.concat([data_reward, data_punish, data_mixed], axis=0)\n",
    "    conditions = ['Reward'] * len(data_reward) + ['Punishment'] * len(data_punish) + ['Mixed'] * len(data_mixed)\n",
    "    \n",
    "\n",
    "    tukey_result = pairwise_tukeyhsd(endog=combined_data, groups=conditions, alpha=0.05)\n",
    "    print(tukey_result)\n",
    "\n",
    "    combined_df = pd.DataFrame({\n",
    "        'Condition': conditions,\n",
    "        col_name: combined_data\n",
    "    })\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='Condition', y=col_name, data=combined_df, palette='Set2')\n",
    "    sns.stripplot(x='Condition', y=col_name, data=combined_df, color='black', jitter=True, alpha=0.5)\n",
    "    \n",
    "    plt.title(f'Comparison of {ylabel} Across Conditions', fontsize=16)\n",
    "    plt.xlabel('Condition', fontsize=12)\n",
    "    plt.ylabel(ylabel, fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anova_only(col_name):\n",
    "\n",
    "    data_reward = df_reward_final[col_name]\n",
    "    data_punish = df_punish_final[col_name]\n",
    "    data_mixed = df_mixed_final[col_name]\n",
    "    \n",
    "\n",
    "    anova_result = f_oneway(data_reward, data_punish, data_mixed)\n",
    "\n",
    "    \n",
    "    if anova_result.pvalue < 0.05:\n",
    "        print(f\"ANOVA result for {col_name}: F={anova_result.statistic}, p={anova_result.pvalue}\")\n",
    "        combined_data = pd.concat([data_reward, data_punish, data_mixed], axis=0)\n",
    "        conditions = ['Reward'] * len(data_reward) + ['Punishment'] * len(data_punish) + ['Mixed'] * len(data_mixed)\n",
    "        \n",
    "\n",
    "        tukey_result = pairwise_tukeyhsd(endog=combined_data, groups=conditions, alpha=0.05)\n",
    "        print(tukey_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_plot(col_name,y_limit):\n",
    "\n",
    "    data_reward = df_reward_final[col_name]\n",
    "    data_punish = df_punish_final[col_name]\n",
    "    data_mixed = df_mixed_final[col_name]\n",
    "    \n",
    "    plt.figure(figsize=(4, 6))\n",
    "    Condition_xaxis = ['Reward','Punishment','Mixed']\n",
    "    data = [data_reward.mean(),data_punish.mean(),data_mixed.mean()]\n",
    "    error = [data_reward.sem(),data_punish.sem(),data_mixed.sem()]\n",
    "    plt.bar(Condition_xaxis,data,yerr=error,capsize=5)\n",
    "    #plt.title(f'Comparison of {col_name} Across Conditions', fontsize=16)\n",
    "    plt.xlabel('Condition', fontsize=12)\n",
    "    plt.ylabel(col_name, fontsize=12)\n",
    "    plt.ylim(y_limit)   \n",
    "    plt.grid(False)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_list = ['Interested','Distressed','Excited','Upset',\n",
    "               'Strong','Guilty','Scared','Hostile',\n",
    "               'Enthusiastic','Proud','Irritable','Alert',\n",
    "               'Ashamed','Inspired','Nervous','Determined',\n",
    "               'Attentive','Jittery','Active','Afraid',\n",
    "               'Pos','Neg','Total']\n",
    "\n",
    "for kk in column_list:\n",
    "    anova_only(kk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emotion value we used (Strong, Determined, Jittery)\n",
    "bar_plot('Jittery',[0,4])\n",
    "bar_plot('Strong',[0,4])\n",
    "bar_plot('Determined',[0,4])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
